{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-10-31T18:28:25.613286Z",
          "iopub.status.busy": "2025-10-31T18:28:25.612871Z",
          "iopub.status.idle": "2025-10-31T18:30:00.517754Z",
          "shell.execute_reply": "2025-10-31T18:30:00.516844Z",
          "shell.execute_reply.started": "2025-10-31T18:28:25.613262Z"
        },
        "trusted": true,
        "id": "eWzjrhXoyHLW",
        "outputId": "8dc95053-13b7-4c80-f444-a1eb98dbf099"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.3)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.57.1-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m960.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Collecting sentence-transformers\n",
            "  Downloading sentence_transformers-5.1.2-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.19.1)\n",
            "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n",
            "  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2025.9.18)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.5)\n",
            "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
            "  Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.15.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.9.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.10)\n",
            "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\n",
            "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\n",
            "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\n",
            "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.2.0)\n",
            "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.2.0)\n",
            "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.8.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\n",
            "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.2.0)\n",
            "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.4.0)\n",
            "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\n",
            "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n",
            "Downloading transformers-4.57.1-py3-none-any.whl (12.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m83.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n",
            "\u001b[?25hDownloading sentence_transformers-5.1.2-py3-none-any.whl (488 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m488.0/488.0 kB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m46.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0mm\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m77.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m70.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m74.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, huggingface-hub, tokenizers, nvidia-cusolver-cu12, transformers, sentence-transformers\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 1.0.0rc2\n",
            "    Uninstalling huggingface-hub-1.0.0rc2:\n",
            "      Successfully uninstalled huggingface-hub-1.0.0rc2\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.21.2\n",
            "    Uninstalling tokenizers-0.21.2:\n",
            "      Successfully uninstalled tokenizers-0.21.2\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.53.3\n",
            "    Uninstalling transformers-4.53.3:\n",
            "      Successfully uninstalled transformers-4.53.3\n",
            "  Attempting uninstall: sentence-transformers\n",
            "    Found existing installation: sentence-transformers 4.1.0\n",
            "    Uninstalling sentence-transformers-4.1.0:\n",
            "      Successfully uninstalled sentence-transformers-4.1.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datasets 4.1.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\n",
            "libcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\n",
            "gradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\n",
            "pylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\n",
            "pylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed huggingface-hub-0.36.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 sentence-transformers-5.1.2 tokenizers-0.22.1 transformers-4.57.1\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade transformers sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from tqdm import tqdm\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"Inference - Generate Predictions\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Fix warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "os.environ['TOKENIZERS_PARALLELISM'] = 'false'"
      ],
      "metadata": {
        "id": "n45fflmhyKLK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CONFIGURATION\n",
        "# ============================================================================\n",
        "\n",
        "CONFIG = {\n",
        "    'model_path': None,\n",
        "\n",
        "    # Model architecture\n",
        "    'model_type': 'heteroscedastic',\n",
        "    'embedding_dim': 768,\n",
        "    'hidden_dims': [512, 256, 128],\n",
        "    'dropout': 0.4,\n",
        "\n",
        "    # Inference settings\n",
        "    'batch_size': 64,\n",
        "    'encoder_name': 'paraphrase-multilingual-mpnet-base-v2',\n",
        "\n",
        "    # Paths\n",
        "    'data_dir': '/kaggle/input/da5401-2025-data-challenge',\n",
        "    'output_dir': '/kaggle/working',\n",
        "}"
      ],
      "metadata": {
        "id": "xlFFagSpyMIR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# MODEL ARCHITECTURE\n",
        "# ============================================================================\n",
        "\n",
        "class StandardMetricMatchingModel(nn.Module):\n",
        "    \"\"\"Standard classification model with LayerNorm (matches training).\"\"\"\n",
        "\n",
        "    def __init__(self, embedding_dim=768, hidden_dims=[512, 256, 128], dropout=0.4):\n",
        "        super().__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.bilinear = nn.Bilinear(embedding_dim, embedding_dim, embedding_dim)\n",
        "\n",
        "        self.fc_layers = nn.ModuleList()\n",
        "        self.layer_norms = nn.ModuleList()\n",
        "        input_dim = embedding_dim * 3\n",
        "\n",
        "        for hidden_dim in hidden_dims:\n",
        "            self.fc_layers.append(nn.Linear(input_dim, hidden_dim))\n",
        "            self.layer_norms.append(nn.LayerNorm(hidden_dim))\n",
        "            input_dim = hidden_dim\n",
        "\n",
        "        self.output = nn.Linear(input_dim, 11)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, metric_emb, text_emb):\n",
        "        bilinear_out = self.bilinear(metric_emb, text_emb)\n",
        "        combined = torch.cat([metric_emb, text_emb, bilinear_out], dim=1)\n",
        "\n",
        "        x = combined\n",
        "        for fc, ln in zip(self.fc_layers[:-1], self.layer_norms[:-1]):\n",
        "            x = fc(x)\n",
        "            x = ln(x)\n",
        "            x = F.relu(x)\n",
        "            x = self.dropout(x)\n",
        "\n",
        "        x = self.fc_layers[-1](x)\n",
        "        x = self.layer_norms[-1](x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        return self.output(x)\n",
        "\n",
        "    def predict_score(self, metric_emb, text_emb):\n",
        "        logits = self.forward(metric_emb, text_emb)\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "        expected_score = torch.sum(probs * torch.arange(11, device=metric_emb.device).float(), dim=1)\n",
        "        # Clamp to valid range [0, 10]\n",
        "        expected_score = torch.clamp(expected_score, 0.0, 10.0)\n",
        "        return expected_score\n",
        "\n",
        "\n",
        "class HeteroscedasticMatchingModel(nn.Module):\n",
        "    \"\"\"Heteroscedastic regression model: predicts mean (mu) and log-variance (logvar).\"\"\"\n",
        "\n",
        "    def __init__(self, embedding_dim=768, hidden_dims=[512, 256, 128], dropout=0.4):\n",
        "        super().__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.bilinear = nn.Bilinear(embedding_dim, embedding_dim, embedding_dim)\n",
        "\n",
        "        self.fc_layers = nn.ModuleList()\n",
        "        self.layer_norms = nn.ModuleList()\n",
        "        input_dim = embedding_dim * 3\n",
        "\n",
        "        for hidden_dim in hidden_dims:\n",
        "            self.fc_layers.append(nn.Linear(input_dim, hidden_dim))\n",
        "            self.layer_norms.append(nn.LayerNorm(hidden_dim))\n",
        "            input_dim = hidden_dim\n",
        "\n",
        "        # Two outputs: mean and log-variance\n",
        "        self.mean_head = nn.Linear(input_dim, 1)\n",
        "        self.logvar_head = nn.Linear(input_dim, 1)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, metric_emb, text_emb):\n",
        "        bilinear_out = self.bilinear(metric_emb, text_emb)\n",
        "        combined = torch.cat([metric_emb, text_emb, bilinear_out], dim=1)\n",
        "\n",
        "        x = combined\n",
        "        for fc, ln in zip(self.fc_layers[:-1], self.layer_norms[:-1]):\n",
        "            x = fc(x)\n",
        "            x = ln(x)\n",
        "            x = F.relu(x)\n",
        "            x = self.dropout(x)\n",
        "\n",
        "        x = self.fc_layers[-1](x)\n",
        "        x = self.layer_norms[-1](x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        mean = self.mean_head(x)\n",
        "        logvar = self.logvar_head(x)\n",
        "\n",
        "        # Clip logvar to prevent extremes (recommended: [-10, 10])\n",
        "        logvar = torch.clamp(logvar, -10.0, 10.0)\n",
        "\n",
        "        return mean.squeeze(-1), logvar.squeeze(-1)\n",
        "\n",
        "    def predict_score(self, metric_emb, text_emb):\n",
        "        \"\"\"Predict expected mean score.\"\"\"\n",
        "        mean, logvar = self.forward(metric_emb, text_emb)\n",
        "        # Clamp mean to valid range [0, 10]\n",
        "        mean = torch.clamp(mean, 0.0, 10.0)\n",
        "        return mean\n",
        "\n",
        "# Alias for backward compatibility\n",
        "MetricMatchingModel = StandardMetricMatchingModel\n",
        "\n",
        "# ============================================================================\n",
        "# DATASET CLASS\n",
        "# ============================================================================\n",
        "\n",
        "class MetricMatchingDataset(Dataset):\n",
        "    def __init__(self, data, metric_embeddings, metric_names_map, text_embeddings):\n",
        "        self.data = data\n",
        "        self.metric_embeddings = torch.FloatTensor(metric_embeddings)\n",
        "        self.metric_names_map = metric_names_map\n",
        "        self.text_embeddings = torch.FloatTensor(text_embeddings)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        record = self.data[idx]\n",
        "        metric_idx = self.metric_names_map[record['metric_name']]\n",
        "        metric_emb = self.metric_embeddings[metric_idx]\n",
        "        text_emb = self.text_embeddings[idx]\n",
        "        return metric_emb, text_emb, idx\n",
        "\n"
      ],
      "metadata": {
        "id": "xqWWXzaTyOq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# LOAD DATA\n",
        "# ============================================================================\n",
        "\n",
        "data_dir = None\n",
        "\n",
        "# Find model\n",
        "model_path = CONFIG.get('model_path')\n",
        "print(f\"Using model: {model_path}\")\n",
        "\n",
        "print(\"\\nLoading data...\")\n",
        "with open(os.path.join(data_dir, 'metric_names.json'), encoding='utf-8') as f:\n",
        "    metric_names = json.load(f)\n",
        "\n",
        "metric_embeddings = np.load(os.path.join(data_dir, 'metric_name_embeddings.npy'))\n",
        "metric_names_map = {name: idx for idx, name in enumerate(metric_names)}\n",
        "\n",
        "print(f\"Loaded {len(metric_names)} metrics\")\n",
        "\n",
        "test_data_path = None\n",
        "\n",
        "\n",
        "if not test_data_path:\n",
        "    raise FileNotFoundError(\"Could not find test_data.json\")\n",
        "\n",
        "print(f\"Loading test data from: {test_data_path}\")\n",
        "with open(test_data_path, encoding='utf-8') as f:\n",
        "    test_data = json.load(f)\n",
        "\n",
        "print(f\"Loaded {len(test_data)} test samples\")\n",
        "\n"
      ],
      "metadata": {
        "id": "nXq4lLXhyTfX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# ENCODE TEXTS\n",
        "# ============================================================================\n",
        "\n",
        "def combine_text_fields(record):\n",
        "    parts = []\n",
        "    if record.get('system_prompt'): parts.append(record['system_prompt'])\n",
        "    if record.get('user_prompt'): parts.append(record['user_prompt'])\n",
        "    elif record.get('prompt'): parts.append(record['prompt'])\n",
        "    if record.get('response'): parts.append(record['response'])\n",
        "    elif record.get('expected_response'): parts.append(record['expected_response'])\n",
        "    return ' '.join(parts)\n",
        "\n",
        "print(\"\\nLoading text encoder...\")\n",
        "text_encoder = SentenceTransformer(CONFIG['encoder_name'])\n",
        "print(\" Successfully loaded encoder\")\n",
        "\n",
        "\n",
        "print(\"Encoding test texts...\")\n",
        "test_texts = [combine_text_fields(rec) for rec in test_data]\n",
        "test_embeddings = text_encoder.encode(\n",
        "    test_texts,\n",
        "    show_progress_bar=True,\n",
        "    batch_size=64,\n",
        "    convert_to_numpy=True\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "ZzoMdl8cyW8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-10-31T11:10:30.648769Z",
          "iopub.status.busy": "2025-10-31T11:10:30.648115Z",
          "iopub.status.idle": "2025-10-31T11:11:28.807697Z",
          "shell.execute_reply": "2025-10-31T11:11:28.807054Z",
          "shell.execute_reply.started": "2025-10-31T11:10:30.648743Z"
        },
        "trusted": true,
        "colab": {
          "referenced_widgets": [
            "48c8712786c34f3d8c48a51c4e5ed4b9",
            "2958e0d4b24e41aea1eca17a90a426f0",
            "6b16772c99894976a50780c70937a658",
            "ef050a4820db4880aba1754bef163e75",
            "2177c0f066544f60975613ea3ddbaeda"
          ]
        },
        "id": "Q4AG1uvwyHLZ",
        "outputId": "58afd4c0-ad34-4615-b2f5-bc6243dfc440"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checking library versions...\n",
            "Current transformers version: 4.57.1\n",
            "✓ Transformers version 4.57.1 should be compatible\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-31 11:10:35.987504: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1761909036.009548     558 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1761909036.016189     558 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Sentence-transformers version: 5.1.2\n",
            "============================================================\n",
            "Kaggle Inference - Generate Predictions\n",
            "============================================================\n",
            "✓ Applied transformers compatibility patch\n",
            "Found data at: /kaggle/input/da5401-2025-data-challenge\n",
            "Found model at: /kaggle/input/da5401-data-challenge-1/best_model.pth\n",
            "Using model: /kaggle/input/da5401-data-challenge-1/best_model.pth\n",
            "\n",
            "Loading data...\n",
            "Loaded 145 metrics\n",
            "Loading test data from: /kaggle/input/da5401-2025-data-challenge/test_data.json\n",
            "Loaded 3638 test samples\n",
            "\n",
            "Loading text encoder...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "48c8712786c34f3d8c48a51c4e5ed4b9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2958e0d4b24e41aea1eca17a90a426f0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6b16772c99894976a50780c70937a658",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ef050a4820db4880aba1754bef163e75",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Successfully loaded encoder\n",
            "Encoding test texts...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2177c0f066544f60975613ea3ddbaeda",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/57 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Loading trained model...\n",
            "============================================================\n",
            "Using device: cuda\n",
            "✓ Loaded model from: /kaggle/input/da5401-data-challenge-1/best_model.pth\n",
            "  (No RMSE metadata available)\n",
            "\n",
            "============================================================\n",
            "Generating predictions...\n",
            "============================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Predicting: 100%|██████████| 29/29 [00:01<00:00, 20.01it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Creating submission file...\n",
            "\n",
            "============================================================\n",
            "✅ Submission saved!\n",
            "============================================================\n",
            "\n",
            "File: /kaggle/working/submission.csv\n",
            "Shape: (3638, 2)\n",
            "\n",
            "Score distribution:\n",
            "score\n",
            "8.0       29\n",
            "9.0     3087\n",
            "10.0     522\n",
            "Name: count, dtype: int64\n",
            "\n",
            "First 20 predictions:\n",
            "    ID  score\n",
            "0    1    9.0\n",
            "1    2    9.0\n",
            "2    3    9.0\n",
            "3    4    9.0\n",
            "4    5    9.0\n",
            "5    6    9.0\n",
            "6    7    9.0\n",
            "7    8   10.0\n",
            "8    9    9.0\n",
            "9   10    9.0\n",
            "10  11    9.0\n",
            "11  12    9.0\n",
            "12  13    9.0\n",
            "13  14    9.0\n",
            "14  15   10.0\n",
            "15  16    9.0\n",
            "16  17   10.0\n",
            "17  18    9.0\n",
            "18  19    9.0\n",
            "19  20    9.0\n",
            "\n",
            "============================================================\n",
            "Done! Download submission.csv from /kaggle/working/\n",
            "============================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CREATE MODEL AND LOAD WEIGHTS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Loading trained model...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Create model matching training architecture\n",
        "if CONFIG['model_type'] == 'heteroscedastic':\n",
        "    print(f\"Creating HeteroscedasticMatchingModel (matches training)\")\n",
        "    model = HeteroscedasticMatchingModel(\n",
        "        embedding_dim=CONFIG['embedding_dim'],\n",
        "        hidden_dims=CONFIG['hidden_dims'],\n",
        "        dropout=CONFIG['dropout']\n",
        "    ).to(device)\n",
        "elif CONFIG['model_type'] == 'standard' or CONFIG['model_type'] == 'default':\n",
        "    print(f\"Creating StandardMetricMatchingModel\")\n",
        "    model = StandardMetricMatchingModel(\n",
        "        embedding_dim=CONFIG['embedding_dim'],\n",
        "        hidden_dims=CONFIG['hidden_dims'],\n",
        "        dropout=CONFIG['dropout']\n",
        "    ).to(device)\n",
        "else:\n",
        "    # For other model types, use StandardMetricMatchingModel as fallback\n",
        "    print(f\"Warning: model_type '{CONFIG['model_type']}' not fully supported, using StandardMetricMatchingModel\")\n",
        "    model = StandardMetricMatchingModel(\n",
        "        embedding_dim=CONFIG['embedding_dim'],\n",
        "        hidden_dims=CONFIG['hidden_dims'],\n",
        "        dropout=CONFIG['dropout']\n",
        "    ).to(device)\n",
        "\n",
        "# Load trained weights - handle both checkpoint dict and state_dict formats\n",
        "checkpoint = torch.load(CONFIG['model_path'], map_location=device, weights_only=False)\n",
        "\n",
        "if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:\n",
        "    # Full checkpoint with metadata\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    if 'best_rmse' in checkpoint:\n",
        "        print(f\"Checkpoint RMSE: {checkpoint['best_rmse']:.4f}\")\n",
        "    if 'epoch' in checkpoint:\n",
        "        print(f\" Checkpoint epoch: {checkpoint['epoch']+1}\")\n",
        "    print(f\"Loaded model from: {CONFIG['model_path']}\")\n",
        "else:\n",
        "    # Just state_dict\n",
        "    model.load_state_dict(checkpoint)\n",
        "    print(f\"Loaded model from: {CONFIG['model_path']}\")\n",
        "    print(\"  (No RMSE metadata available)\")\n",
        "\n",
        "# ============================================================================\n",
        "# GENERATE PREDICTIONS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Generating predictions...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "test_dataset = MetricMatchingDataset(test_data, metric_embeddings, metric_names_map, test_embeddings)\n",
        "test_loader = DataLoader(test_dataset, batch_size=CONFIG['batch_size'], shuffle=False)\n",
        "\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_indices = []\n",
        "\n",
        "# Use inference_mode (more efficient) + autocast for memory savings\n",
        "use_amp = torch.cuda.is_available()\n",
        "with torch.inference_mode():\n",
        "    if use_amp:\n",
        "        # Mixed precision inference (50% memory savings)\n",
        "        with torch.cuda.amp.autocast():\n",
        "            for metric_emb, text_emb, idx in tqdm(test_loader, desc=\"Predicting\"):\n",
        "                metric_emb = metric_emb.to(device)\n",
        "                text_emb = text_emb.to(device)\n",
        "                scores = model.predict_score(metric_emb, text_emb)\n",
        "                all_preds.extend(scores.cpu().numpy())\n",
        "                all_indices.extend(idx.cpu().numpy())\n",
        "    else:\n",
        "        # Standard precision (CPU)\n",
        "        for metric_emb, text_emb, idx in tqdm(test_loader, desc=\"Predicting\"):\n",
        "            metric_emb = metric_emb.to(device)\n",
        "            text_emb = text_emb.to(device)\n",
        "            scores = model.predict_score(metric_emb, text_emb)\n",
        "            all_preds.extend(scores.cpu().numpy())\n",
        "            all_indices.extend(idx.cpu().numpy())\n",
        "\n",
        "# Ensure predictions are in valid range and rounded\n",
        "all_preds = np.array(all_preds)\n",
        "all_preds = np.round(all_preds).clip(0, 10)  # Keep as float to match sample format (7.0 instead of 7)\n",
        "\n",
        "# ============================================================================\n",
        "# CREATE SUBMISSION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\nCreating submission file...\")\n",
        "\n",
        "# Get IDs from test data\n",
        "# Since test_data.json doesn't have ID field, use sequential indices (0, 1, 2, ...)\n",
        "# The dataset preserves order (shuffle=False), so indices should be in order\n",
        "# Create submission with sequential IDs matching the test_data order\n",
        "\n",
        "# Sort predictions by index to ensure correct order\n",
        "sorted_pairs = sorted(zip(all_indices, all_preds))\n",
        "submission_scores = [pred for _, pred in sorted_pairs]\n",
        "\n",
        "# Create sequential IDs (1, 2, 3, ..., len(test_data))\n",
        "# Sample submission uses 1-based indexing\n",
        "submission_ids = list(range(1, len(test_data) + 1))\n",
        "\n",
        "# Verify we have the correct number of predictions\n",
        "if len(submission_scores) != len(test_data):\n",
        "    print(f\"Warning: Expected {len(test_data)} predictions, got {len(submission_scores)}\")\n",
        "    # Pad or truncate if needed\n",
        "    if len(submission_scores) < len(test_data):\n",
        "        submission_scores.extend([float(np.round(np.mean(all_preds)))] * (len(test_data) - len(submission_scores)))\n",
        "    else:\n",
        "        submission_scores = submission_scores[:len(test_data)]\n",
        "\n",
        "submission = pd.DataFrame({\n",
        "    'ID': submission_ids,\n",
        "    'score': submission_scores\n",
        "})\n",
        "\n",
        "# Save submission\n",
        "output_path = os.path.join(CONFIG['output_dir'], 'submission.csv')\n",
        "submission.to_csv(output_path, index=False)\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\" Submission saved!\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"\\nFile: {output_path}\")\n",
        "print(f\"Shape: {submission.shape}\")\n",
        "print(f\"\\nScore distribution:\")\n",
        "print(submission['score'].value_counts().sort_index())\n",
        "print(f\"\\nFirst 20 predictions:\")\n",
        "print(submission.head(20))\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Done! Download submission.csv from /kaggle/working/\")\n",
        "print(\"=\"*60)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "databundleVersionId": 14294892,
          "sourceId": 118082,
          "sourceType": "competition"
        },
        {
          "sourceId": 272173550,
          "sourceType": "kernelVersion"
        },
        {
          "sourceId": 272350502,
          "sourceType": "kernelVersion"
        }
      ],
      "dockerImageVersionId": 31154,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}